{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memphis, Tennessee\n",
    "\n",
    "https://overpass-api.de/api/map?bbox=-90.0783,35.0688,-89.8654,35.1968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import sqlite3\n",
    "from collections import defaultdict\n",
    "import cerberus\n",
    "import schema\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"map.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find and count all the top level tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 22448,\n",
      " 'meta': 1,\n",
      " 'nd': 1464119,\n",
      " 'node': 1331083,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 903,\n",
      " 'tag': 218854,\n",
      " 'way': 118315}\n"
     ]
    }
   ],
   "source": [
    "#Find top level tags in the data set and count them, code taken from case study. \n",
    "\n",
    "tags = {}\n",
    "\n",
    "def count_tags(filename):\n",
    "    for event, elem in ET.iterparse(filename ,events=(\"start\",)):\n",
    "        if elem.tag in tags.keys(): #Check if the tag is already in tags\n",
    "            tags[elem.tag] += 1     #Add 1 to the count if it is. \n",
    "        else:\n",
    "            tags[elem.tag] = 1      #If it isnt, set the count to 1\n",
    "    return tags\n",
    "\n",
    "with open('map.osm', 'r') as mapfile:\n",
    "    pprint.pprint(count_tags(mapfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check K values for problem characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 163145, 'lower_colon': 53209, 'other': 2500, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "#Code used in Case Study\n",
    "#Checking the k values for each tag for problem characters\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def check_k(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if lower.match(element.attrib['k']):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys[\"lower_colon\"] +=1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys [\"other\"] += 1\n",
    "    pass\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = check_k(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "with open('map.osm', 'r') as mapfile:\n",
    "    keys = process_map(mapfile)\n",
    "    pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit and correct street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covington Pike -> Covington Pike\n",
      "Mississippi -> Mississippi\n",
      "Perkins Extended -> Perkins Extended\n",
      "Jackson -> Jackson\n",
      "Front -> Front\n",
      "Avon Rd -> Avon Road \n",
      "E Brookhaven Cir -> E Brookhaven Circle \n",
      "Poplar -> Poplar\n",
      "Ridge Lake Blvd -> Ridge Lake Boulevard\n",
      "B.B. King Blvd -> B.B. King Boulevard\n",
      "Lamar Ave -> Lamar Avenue \n",
      "Shadyac Ave -> Shadyac Avenue \n",
      "W G E Patterson Ave -> W G East Patterson Ave\n",
      "Chelsea Ave -> Chelsea Avenue \n",
      "Central Ave -> Central Avenue \n",
      "Lynnfield Road Suite 236 -> Lynnfield Road Suite 236\n",
      "Main -> Main\n",
      "Clarke Rd. -> Clarke Road .\n"
     ]
    }
   ],
   "source": [
    "#Auditing street Names and correcting any issues. Used in Case Study\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\",\"Freeway\",\"Circle\",\"Strand\",\"Sterling\",\"Way\",\"Highway\",\n",
    "            \"Terrace\",\"South\",\"East\",\"West\",\"North\", \"Cove\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s:s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    with open ('map.osm', 'r') as mapfile:\n",
    "        street_types = defaultdict(set)\n",
    "        for event, elem in ET.iterparse(mapfile, events=(\"start\", )):\n",
    "            if elem.tag == \"way\" or elem.tag == \"node\":\n",
    "                for tag in elem.iter(\"tag\"):\n",
    "                    if is_street_name(tag):\n",
    "                        audit_street_type(street_types, tag.attrib['v'])\n",
    "        return street_types\n",
    "        \n",
    "audit()\n",
    "\n",
    "#Correcting Street Types\n",
    "\n",
    "mapping = {\n",
    "            \" St \": \" Street \",\n",
    "            \" St.\": \" Street \",\n",
    "            \" Rd.\": \" Road \",\n",
    "            \" Rd \": \" Road \",\n",
    "            \" Rd\": \" Road \",\n",
    "            \" Ave \": \" Avenue \", \n",
    "            \" Ave.\": \" Avenue \",\n",
    "            \" Av \": \" Avenue \",\n",
    "            \" Ave\" : \" Avenue \",\n",
    "            \" Dr \": \" Drive \",\n",
    "            \" Dr.\": \" Drive\",\n",
    "            \" Blvd \": \" Boulevard \",\n",
    "            \" Blvd\": \" Boulevard\",\n",
    "            \" Blvd.\": \" Boulevard\",\n",
    "            \" Ct \": \" Centre \",\n",
    "            \" Ctr\": \" Centre\",\n",
    "            \" Pl \": \" Place \",\n",
    "            \" Ln \": \" Lane \",\n",
    "            \" Cir \": \" Circle \",\n",
    "            \" Cir\" : \" Circle \",\n",
    "            \" Wy\": \" Way \",\n",
    "            \" S \": \" South \",\n",
    "            \" E \": \" East \",\n",
    "            \" W \": \" West \",\n",
    "            \" N \": \"North\"\n",
    "}\n",
    "\n",
    "#Update function that will also be used when preparing the data for SQL. \n",
    "def update_name(name, mapping):\n",
    "    for key, value in mapping.iteritems():\n",
    "        if key in name:\n",
    "            return name.replace(key, value)\n",
    "    return name\n",
    "\n",
    "with open ('map.osm', 'r') as mapfile:\n",
    "    s_types = audit()\n",
    "    \n",
    "    for s_type, ways in s_types.iteritems():\n",
    "        for name in ways:\n",
    "            correct_name = update_name(name, mapping)\n",
    "            print name, \"->\", correct_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like abbreviations are the biggest issue with the street names. I'll use the update_name function when I prepare the data for SQL to correct the abbreviations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit and correct zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38111 -> 38111\n",
      "38112 -> 38112\n",
      "38114 -> 38114\n",
      "38115 -> 38115\n",
      "38117 -> 38117\n",
      "38118 -> 38118\n",
      "38119 -> 38119\n",
      "38132 -> 38132\n",
      "38134 -> 38134\n",
      "3813 -> 0\n",
      "3951 -> 0\n",
      "38107 -> 38107\n",
      "38106 -> 38106\n",
      "38105 -> 38105\n",
      "38104 -> 38104\n",
      "38103 -> 38103\n",
      "38109 -> 38109\n",
      "38108 -> 38108\n",
      "38128 -> 38128\n",
      "38152 -> 38152\n",
      "38163 -> 38163\n",
      "38120 -> 38120\n",
      "38122 -> 38122\n",
      "38127 -> 38127\n",
      "38126 -> 38126\n"
     ]
    }
   ],
   "source": [
    "#Auditing zip codes and correcting errors, this is an alteration of the case study script for auditing street names. \n",
    "\n",
    "zip_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "zip_types = defaultdict(set)\n",
    "expected = []\n",
    "\n",
    "def audit_zip_code(zip_types, zip_name):\n",
    "    m = zip_type_re.search(zip_name)\n",
    "    if m:\n",
    "        zip_type = m.group()\n",
    "        if zip_type not in expected:\n",
    "            zip_types[zip_type].add(zip_name)\n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s:s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v)\n",
    "\n",
    "def is_zip_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zip():\n",
    "    with open ('map.osm', 'r') as mapfile:\n",
    "        for event, elem in ET.iterparse(mapfile, events=(\"start\", )):\n",
    "            if elem.tag == \"way\" or elem.tag == \"node\":\n",
    "                for tag in elem.iter(\"tag\"):\n",
    "                    if is_zip_code(tag):\n",
    "                        audit_zip_code(zip_types, tag.attrib['v'])\n",
    "        return zip_types\n",
    "        \n",
    "def update_zipcode(zipcode): \n",
    "    if len(str(zipcode))<5:\n",
    "        zipcode = 0\n",
    "    return zipcode\n",
    "\n",
    "with open ('map.osm', 'r') as mapfile:\n",
    "    s_types = audit_zip()\n",
    "    \n",
    "    for s_type, ways in s_types.iteritems():\n",
    "        for name in ways:\n",
    "            correct_name = update_zipcode(name)\n",
    "            print name, \"->\", correct_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two zipcodes that are incomplete. I'll convert those to 0 when preparing for SQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code used in Case Study\n",
    "OSM_PATH = open(\"map.osm\", \"r\")\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in NODE_FIELDS:\n",
    "                node_attribs[attrib] = element.attrib[attrib]\n",
    "        \n",
    "        for child in element:\n",
    "            node_tag = {}\n",
    "            if LOWER_COLON.match(child.attrib['k']):\n",
    "                node_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                node_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                node_tag['id'] = element.attrib['id']\n",
    "                node_tag['value'] = child.attrib['v']\n",
    "                #This is where I will update the street abreviations and zip code errors. \n",
    "                if child.attrib['k'] == \"addr:street\":\n",
    "                    node_tag['value'] = update_name(child.attrib['v'], mapping)\n",
    "                elif child.attrib['k'] == \"addr:postcode\":\n",
    "                    node_tag['value'] = update_zipcode(child.attrib['v'])\n",
    "                tags.append(node_tag)\n",
    "            elif PROBLEMCHARS.match(child.attrib['k']): #Check for problem characters and discard if true.\n",
    "                continue\n",
    "            else:\n",
    "                node_tag['type'] = 'regular'\n",
    "                node_tag['key'] = child.attrib['k']\n",
    "                node_tag['id'] = element.attrib['id']\n",
    "                node_tag['value'] = child.attrib['v']\n",
    "                tags.append(node_tag)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    elif element.tag == 'way':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in WAY_FIELDS:\n",
    "                way_attribs[attrib] = element.attrib[attrib]\n",
    "        \n",
    "        position = 0\n",
    "        for child in element:\n",
    "            way_tag = {}\n",
    "            way_node = {}\n",
    "            \n",
    "            if child.tag == 'tag':\n",
    "                if LOWER_COLON.match(child.attrib['k']):\n",
    "                    way_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                    way_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    #This is where I will update the street abreviations and zip code errors. \n",
    "                    if child.attrib['k'] == \"addr:street\":\n",
    "                        way_tag['value'] = update_name(child.attrib['v'], mapping)\n",
    "                    elif child.attrib['k'] == \"addr:postcode\":\n",
    "                        way_tag['value'] = update_zipcode(child.attrib['v'])\n",
    "                    tags.append(way_tag)\n",
    "                elif PROBLEMCHARS.match(child.attrib['k']): #Check for problem characters and discard if true. \n",
    "                    continue\n",
    "                else:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = child.attrib['k']\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    tags.append(way_tag)\n",
    "                    \n",
    "            elif child.tag == 'nd':\n",
    "                way_node['id'] = element.attrib['id']\n",
    "                way_node['node_id'] = child.attrib['ref']\n",
    "                way_node['position'] = position\n",
    "                position += 1\n",
    "                way_nodes.append(way_node)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "process_map(OSM_PATH, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\"map.db\")\n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Nodes\n",
    "#Checking if the table exists and dropping it if it does, then create the table. \n",
    "cur.execute(\"DROP TABLE IF EXISTS nodes;\")\n",
    "db.commit()\n",
    "cur.execute(\"CREATE TABLE nodes (id INTEGER PRIMARY KEY NOT NULL,lat REAL,lon REAL,user TEXT,uid INTEGER,version INTEGER,changeset INTEGER,timestamp TEXT);\")\n",
    "db.commit()\n",
    "\n",
    "#Read the csv file\n",
    "with open('nodes.csv','rb') as f: \n",
    "    dr = csv.DictReader(f)\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['lat'].decode(\"utf-8\"),i['lon'].decode(\"utf-8\"),i['user'].decode(\"utf-8\"),i['uid'].decode(\"utf-8\"),i['version'].decode(\"utf-8\"),i['changeset'].decode(\"utf-8\"),i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "#Insert the data into the table\n",
    "cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?,?,?);\", to_db)\n",
    "db.commit()\n",
    "\n",
    "#Create Nodes_Tags\n",
    "cur.execute(\"DROP TABLE IF EXISTS nodes_tags;\")\n",
    "db.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE nodes_tags (id INTEGER, key TEXT, value TEXT, type TEXT, FOREIGN KEY (id) REFERENCES nodes(id))\")\n",
    "db.commit()\n",
    "\n",
    "with open('nodes_tags.csv', 'rb') as f:\n",
    "    dr = csv.DictReader(f)\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"),i['type'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags (id, key, value, type) VALUES (?,?,?,?);\", to_db)\n",
    "db.commit()\n",
    "\n",
    "#Create Ways\n",
    "cur.execute(\"DROP TABLE IF EXISTS ways;\")\n",
    "db.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways(id INTEGER PRIMARY KEY NOT NULL,user TEXT,uid INTEGER,version TEXT,changeset INTEGER,timestamp TEXT);\")\n",
    "db.commit()\n",
    "\n",
    "with open('ways.csv','rb') as f: \n",
    "    dr = csv.DictReader(f)\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['user'].decode(\"utf-8\"),i['uid'].decode(\"utf-8\"),i['version'].decode(\"utf-8\"),i['changeset'].decode(\"utf-8\"),i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways (id, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?);\", to_db)\n",
    "db.commit()\n",
    "\n",
    "#Create Ways_Nodes\n",
    "cur.execute(\"DROP TABLE IF EXISTS ways_nodes;\")\n",
    "db.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_nodes (id INTEGER NOT NULL,node_id INTEGER NOT NULL,position INTEGER NOT NULL,FOREIGN KEY (id) REFERENCES ways(id),FOREIGN KEY (node_id) REFERENCES nodes(id));\")\n",
    "db.commit()\n",
    "\n",
    "with open('ways_nodes.csv','rb') as f: \n",
    "    dr = csv.DictReader(f)\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['node_id'].decode(\"utf-8\"),i['position'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes (id, node_id, position) VALUES (?,?,?);\", to_db)\n",
    "db.commit()\n",
    "\n",
    "#Create Ways_Tags\n",
    "cur.execute(\"DROP TABLE IF EXISTS ways_tags;\");\n",
    "db.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_tags (id INTEGER NOT NULL,key TEXT NOT NULL,value TEXT NOT NULL,type TEXT,FOREIGN KEY (id) REFERENCES ways(id));\")\n",
    "db.commit()\n",
    "\n",
    "with open('ways_tags.csv','rb') as f: \n",
    "    dr = csv.DictReader(f)\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"),i['type'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags (id, key, value, type) VALUES (?,?,?,?);\", to_db)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map.osm ..... 257 MB\n",
      "sample.osm ..... 26 MB\n",
      "map.db ..... 127 MB\n",
      "nodes.csv ..... 105 MB\n",
      "nodes_tags.csv ..... 0 MB\n",
      "ways.csv ..... 6 MB\n",
      "ways_tags.csv ..... 6 MB\n",
      "ways_nodes.csv ..... 35 MB\n"
     ]
    }
   ],
   "source": [
    "print 'map.osm', '.....', (os.path.getsize(\"map.osm\")/(1024*1024)), 'MB'\n",
    "print 'sample.osm', '.....', (os.path.getsize(\"sample.osm\")/(1024*1024)), 'MB'\n",
    "print 'map.db', '.....', (os.path.getsize(\"map.db\")/(1024*1024)), 'MB'\n",
    "print 'nodes.csv', '.....', (os.path.getsize(\"nodes.csv\")/(1024*1024)), 'MB'\n",
    "print 'nodes_tags.csv', '.....', (os.path.getsize(\"nodes_tags.csv\")/(1024*1024)), 'MB'\n",
    "print 'ways.csv', '.....', (os.path.getsize(\"ways.csv\")/(1024*1024)), 'MB'\n",
    "print 'ways_tags.csv', '.....', (os.path.getsize(\"ways_tags.csv\")/(1024*1024)), 'MB'\n",
    "print 'ways_nodes.csv', '.....', (os.path.getsize(\"ways_nodes.csv\")/(1024*1024)), 'MB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Nodes and Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1331083,)]\n"
     ]
    }
   ],
   "source": [
    "#Number of nodes\n",
    "query = cur.execute('SELECT COUNT(*) FROM nodes')\n",
    "print query.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(118315,)]\n"
     ]
    }
   ],
   "source": [
    "#Number of ways\n",
    "query = cur.execute('SELECT COUNT(*) FROM ways')\n",
    "print query.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'regular', 13225),\n",
      " (u'gnis', 4120),\n",
      " (u'addr', 968),\n",
      " (u'brand', 134),\n",
      " (u'tower', 39),\n",
      " (u'ref', 15),\n",
      " (u'contact', 15),\n",
      " (u'service', 12),\n",
      " (u'communication', 11),\n",
      " (u'social_facility', 9),\n",
      " (u'operator', 9),\n",
      " (u'historic', 9),\n",
      " (u'was', 8),\n",
      " (u'payment', 8),\n",
      " (u'traffic_signals', 4),\n",
      " (u'source', 4),\n",
      " (u'name', 4),\n",
      " (u'socket', 3),\n",
      " (u'nrhp', 3),\n",
      " (u'healthcare', 3),\n",
      " (u'fuel', 3),\n",
      " (u'toilets', 2),\n",
      " (u'removed', 2),\n",
      " (u'railway', 2),\n",
      " (u'internet_access', 2),\n",
      " (u'demolished', 2),\n",
      " (u'heritage', 1),\n",
      " (u'disused', 1),\n",
      " (u'description', 1),\n",
      " (u'census', 1)]\n"
     ]
    }
   ],
   "source": [
    "query = cur.execute('SELECT type , COUNT(*) AS num FROM nodes_tags GROUP BY type ORDER BY num DESC;')\n",
    "pprint.pprint(query.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694,)\n"
     ]
    }
   ],
   "source": [
    "query = cur.execute('SELECT COUNT(distinct(uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways)')\n",
    "pprint.pprint(query.fetchone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User with the most submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'OSM901', 1291600)]\n"
     ]
    }
   ],
   "source": [
    "query = cur.execute('SELECT e.user, COUNT(*) AS num FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) AS e GROUP BY user ORDER BY num DESC LIMIT 1;')\n",
    "pprint.pprint(query.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number and type of religious locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'christian', 724),\n",
      " (u'jewish', 7),\n",
      " (u'muslim', 5),\n",
      " (u'multifaith', 1),\n",
      " (u'hindu', 1)]\n"
     ]
    }
   ],
   "source": [
    "query= cur.execute(\"SELECT value, COUNT(*) AS num FROM (SELECT key,value FROM nodes_tags UNION ALL SELECT key,value FROM ways_tags) AS e WHERE key='religion' GROUP BY value ORDER BY num DESC;\")\n",
    "pprint.pprint(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Resturaunts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'burger', 18),\n",
      " (u'american', 12),\n",
      " (u'sandwich', 8),\n",
      " (u'mexican', 8),\n",
      " (u'chicken', 8),\n",
      " (u'pizza', 7),\n",
      " (u'coffee_shop', 7),\n",
      " (u'japanese', 4),\n",
      " (u'italian', 4),\n",
      " (u'barbecue', 4),\n",
      " (u'tex-mex', 3),\n",
      " (u'seafood', 3),\n",
      " (u'ice_cream', 3),\n",
      " (u'regional', 2),\n",
      " (u'chinese', 2),\n",
      " (u'asian', 2),\n",
      " (u'wings', 1),\n",
      " (u'vietnamese', 1),\n",
      " (u'thai', 1),\n",
      " (u'steak_house', 1),\n",
      " (u'southern;breakfast', 1),\n",
      " (u'pretzel', 1),\n",
      " (u'pizza;barbecue;steak;southern;breakfast;lunch', 1),\n",
      " (u'mediterranean;korean;sandwich', 1),\n",
      " (u'gastropub', 1),\n",
      " (u'donut', 1),\n",
      " (u'diner', 1),\n",
      " (u'cookies', 1),\n",
      " (u'coffee_shop;southern', 1),\n",
      " (u'coffee;tea', 1),\n",
      " (u'chinese;sushi', 1),\n",
      " (u'chinese;buffet', 1),\n",
      " (u'cake;bagel;coffee_shop', 1),\n",
      " (u'breakfast;pancake', 1),\n",
      " (u'breakfast;coffee_shop', 1),\n",
      " (u'breakfast', 1),\n",
      " (u'bar;hotdogs', 1),\n",
      " (u'arab', 1),\n",
      " (u'american;steak', 1),\n",
      " (u'african', 1),\n",
      " (u'Club_and_Southern_Food', 1),\n",
      " (u'Bar_and_Pub_food', 1),\n",
      " (u'BBQ', 1)]\n"
     ]
    }
   ],
   "source": [
    "query=cur.execute(\"SELECT value, COUNT(*) AS num FROM (SELECT key,value FROM nodes_tags UNION ALL SELECT key,value FROM ways_tags) AS e WHERE e.key LIKE '%cuisine%' GROUP BY value ORDER BY num desc;\")\n",
    "pprint.pprint(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'place_of_worship', 688),\n",
      " (u'school', 171),\n",
      " (u'restaurant', 87),\n",
      " (u'bicycle_rental', 70),\n",
      " (u'fast_food', 22),\n",
      " (u'bar', 21),\n",
      " (u'fuel', 20),\n",
      " (u'library', 15),\n",
      " (u'cafe', 13),\n",
      " (u'post_office', 10),\n",
      " (u'social_facility', 9),\n",
      " (u'grave_yard', 8),\n",
      " (u'parking', 7),\n",
      " (u'vending_machine', 6),\n",
      " (u'toilets', 6),\n",
      " (u'theatre', 6),\n",
      " (u'pharmacy', 6),\n",
      " (u'bench', 6),\n",
      " (u'pub', 5),\n",
      " (u'kindergarten', 5),\n",
      " (u'fountain', 4),\n",
      " (u'clinic', 4),\n",
      " (u'car_rental', 4),\n",
      " (u'bicycle_repair_station', 4),\n",
      " (u'fire_station', 3),\n",
      " (u'doctors', 3),\n",
      " (u'community_centre', 3),\n",
      " (u'bank', 3),\n",
      " (u'atm', 3),\n",
      " (u'waste_basket', 2),\n",
      " (u'university', 2),\n",
      " (u'research_institute', 2),\n",
      " (u'public_building', 2),\n",
      " (u'police', 2),\n",
      " (u'nursing_home', 2),\n",
      " (u'nightclub', 2),\n",
      " (u'ice_cream', 2),\n",
      " (u'hospital', 2),\n",
      " (u'courthouse', 2),\n",
      " (u'college', 2),\n",
      " (u'charging_station', 2),\n",
      " (u'car_wash', 2),\n",
      " (u'bicycle_parking', 2),\n",
      " (u'veterinary', 1),\n",
      " (u'shelter', 1),\n",
      " (u'prison', 1),\n",
      " (u'marketplace', 1),\n",
      " (u'dentist', 1),\n",
      " (u'clock', 1),\n",
      " (u'childcare', 1),\n",
      " (u'bus_station', 1),\n",
      " (u'bbq', 1)]\n"
     ]
    }
   ],
   "source": [
    "query=cur.execute(\"SELECT value, COUNT(*) AS num FROM nodes_tags WHERE key='amenity' GROUP BY value ORDER BY num DESC;\")\n",
    "pprint.pprint(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
